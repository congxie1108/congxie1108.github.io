<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="content-type" content="text/html; charset=UTF-8">
		<title>Homepage</title>
		<link type="text/css" rel="stylesheet" href="style_classic.css">
	<body>
		<div id="main">
			<div id="content" class="clearfix">
				<div id="avatar">
					<img src="images/user/avatar.png" alt="avatar" width="200" height="200" />
				</div>
				<div id="namecard">
					<div class="top">
						<div id="home">
							<h1 style="font-size:36pt;"><strong>Cong Xie</strong></h1>
							<div>
								<br>
								<p>I am working as a Research Scientist at ByteDance, focusing on accelerating the systems for large-scale machine learning, especially the training of foundational large language models. I establish solutions with system-algorithm co-designs for faster machine learning, with both promising performance in practice, and rigorous guarantees in theory. </p>
								<br>
								<p>I obtained my Ph.D. from University of Illinois at Urbana Champaign, co-advised by Prof. <a href="http://indy.cs.illinois.edu">Indranil Gupta</a> and Prof. <a href="http://sanmi.cs.illinois.edu/">Oluwasanmi Koyejo</a>. </p>
								
								<table style="font-size:18px; line-height:20px;width: 560px;margin-top: 10px;">
										<tr>
												<td  valign="top" style="font-weight: bold;"><a href="cv/cv.pdf">CV (updated in Nov 2024)</a></td>
										</tr>
										<tr>
											<td  valign="top" style="font-weight: bold;"><a href="https://scholar.google.com/citations?user=pIPJUJMAAAAJ&hl=en">Google Scholar</a></td>
									</tr>
										<tr>
												<td style="font-weight: bold;">Contact Info</td>
												<td>Email: xcgoner1108 AT gmail.com</td>
										</tr>
								</table>
							<br />
							</div>
						</div>
					</div>
				</div>
				<hr />
				<div id="interest">
					<h2>Research Interest</h2>
					<div>
						<ul>
							<li>Large-scale and Distributed Machine Learning</li>
							<li>System-algorithm Co-design for Machine Learning</li>
							<li>Efficient Machine Learning</li>
							<li>Non-convex Optimization</li>
						</ul>
					</div>
				</div>
				<hr />
				<div id="publication">
					<h2>Publication</h2>
					<ul>
						<li>
							<b>SDP4Bit: Toward 4-bit Communication Quantization in Sharded Data Parallelism for LLM Training.</b><br/>
							Jinda Jia, <b>Cong Xie</b>, Hanlin Lu, Daoce Wang, Hao Feng, Chengming Zhang, Baixi Sun, Haibin Lin, Zhi Zhang, Xin Liu, Dingwen Tao. 
							<br/>
							<i>Advances in Neural Information Processing Systems</i>
							(NeurIPS) 2024.
							<p> </p>
							<br/>
						</li>
						<li>
							<b>MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs.</b><br/>
							Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, <b>Cong Xie</b>, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu. 
							<br/>
							<i>21st USENIX Symposium on Networked Systems Design and Implementation (NSDI) 2024.</i>
							<p> </p>
							<br/>
						</li>
						<li>
							<b>SAPipe: Staleness-Aware Pipeline for Data Parallel DNN Training.</b><br/>
							Yangrui Chen, <b>Cong Xie</b>, Meng Ma, Juncheng Gu, Yanghua Peng, Haibin Lin, Chuan Wu, and Yibo Zhu. 
							<br/>
							<i>Advances in Neural Information Processing Systems</i>
							(NeurIPS) 2022.
							<p> </p>
							<br/>
						</li>
						<li>
							<b>ZenoPS: A Distributed Learning System Integrating Communication Efficiency and Security.</b><br/>
							<b>Cong Xie</b>, Sanmi Koyejo, Indranil Gupta. 
							<br/>
							<i>MDPI Algorithms 15.7 (2022)</i>
							<p> </p>
							<br/>
						</li>
						<li>
							<b>CSER: Communication-efficient SGD with Error Reset.</b><br/>
							<b>Cong Xie</b>
							, Shuai Zheng, Sanmi Koyejo, Indranil Gupta, Mu Li, Haibin Lin. 
							<br/>
							<i>Advances in Neural Information Processing Systems</i>
							(NeurIPS) 2020.
							<p> </p>
							<br/>
						</li>
						<li>
							<b>Zeno++: Robust Fully Asynchronous SGD.</b><br/>
							<b>Cong Xie</b>
							, Sanmi Koyejo, Indranil Gupta. 
							<br/>
							<i>International Conference on Machine Learning</i>
							(ICML) 2020.
							<p> </p>
							<br/>
						</li>
						<li>
							<b>Local AdaAlter: Communication-Efficient Stochastic Gradient Descent with Adaptive Learning Rates.</b><br/>
							<b>Cong Xie</b>
							, Sanmi Koyejo, Indranil Gupta, Haibin Lin. 
							<br/>
							<i>NeurIPS workshop on Optimization for Machine Learning</i>
							(OPT) 2020.
							<br/>
							<a href="https://arxiv.org/abs/1911.09030">https://arxiv.org/abs/1911.09030</a>
							<p> </p>
							<br/>
						</li>
						<li>
							<b>Asynchronous Federated Optimization.</b><br/>
							<b>Cong Xie</b>
							, Sanmi Koyejo, Indranil Gupta. 
							<br/>
							<i>NeurIPS workshop on Optimization for Machine Learning</i>
							(OPT) 2020.
							<br/>
							<a href="https://arxiv.org/abs/1903.03934">https://arxiv.org/abs/1903.03934</a>
							<p> </p>
							<br/>
						</li>
						<li>
							<b>Baechi: Fast Device Placement of Machine Learning Graphs.</b><br/>
							Beomyeol Jeon, Linda Cai, Pallavi Srivastava, Jintao Jiang, Xiaolan Ke, Yitao Meng, <b>Cong Xie</b>, Indranil Gupta. 
							<br/>
							<i>Proc. ACM Symposium on Cloud Computing</i>
							(ACM SoCC), 2020.
							<br/>
							<a href="https://arxiv.org/abs/1903.03934">https://arxiv.org/abs/1903.03934</a>
							<p> </p>
							<br/>
						</li>
                        <li>
							<b>SLSGD: Secure and Efficient Distributed On-device Machine Learning.</b><br/>
							<b>Cong Xie</b>
							, Sanmi Koyejo, Indranil Gupta. 
							<br/>
							<i>European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</i>
							(ECML PKDD) 2019.
							<p> </p>
							<br/>
						</li>
                        <li>
							<b>Fall of Empires: Breaking Byzantine-tolerant SGD by Inner Product Manipulation.</b><br/>
							<b>Cong Xie</b>
							, Sanmi Koyejo, Indranil Gupta. 
							<br/>
							<i>Uncertainty in Artificial Intelligence</i>
							(UAI) 2019.
							<p> </p>
							<br/>
						</li>
                        <li>
							<b>Zeno: Distributed Stochastic Gradient Descent with Suspicion-based Fault-tolerance.</b><br/>
							<b>Cong Xie</b>
							, Sanmi Koyejo, Indranil Gupta. 
							<br/>
							<i>International Conference on Machine Learning</i>
							(ICML) 2019.
							<p> </p>
							<br/>
						</li>
						<li>
							<b>Distributed Power-Law Graph Computing: Theoretical and Empirical Analysis.</b><br/>
							<b>Cong Xie</b>
							, Ling Yan, Wu-Jun Li, and Zhihua Zhang. 
							<br/>
							<i> In Proceedings of Conference on Neural Information Processing Systems</i>
							(NeurIPS) 2014.
							<br/>
							<a href="papers/nips2014.pdf">PDF</a>,
							<a href="https://github.com/xcgoner/powerlore">Code</a>
							<p> </p>
							<br/>
						</li>
						<li>
							<b>A Scalable and Extensible Framework for Superposition-Structured Models.</b><br/>
							Shenjian Zhao, 
							<b>Cong Xie</b>
							, and Zhihua Zhang. 
							<br/>
							<i> The Thirtieth Conference on Artificial Intelligence</i>
							(AAAI-16) 2015. 
							<p> </p>
							<br/>
						</li>
						<li>
							<b>Wishart Mechanism for Differentially Private Principle Components Analysis.</b><br/>
							Wuxuan Jiang, 
							<b>Cong Xie</b>
							, and Zhihua Zhang. 
							<br/>
							<i> The Thirtieth Conference on Artificial Intelligence</i>
							(AAAI-16) 2015. 
							<br/>
							<a href="http://arxiv.org/abs/1511.05680">http://arxiv.org/abs/1511.05680</a>
							<p> </p>
							<br/>
						</li>
						<li>
							<b>Feature Extraction and Ensemble Decision Tree Classifier in Plant Failure Detection.</b><br/>
							<b>Cong Xie</b>
							, Donglin Yang, Yixiang Huang, and Donglai Sun. 
							<br/>
							<i> Annual Conference of the Prognostics and Health Management Society</i>
							(IEEE PHM2015 Data Challenge Winner Paper) 2015.
							<br/>
							<a href="papers/PHM2015_Maxtropy.pdf">PDF</a>,
							<a href="code/phm15_code_pub.zip">Code</a>
							<p> </p>
							<br/>
						</li>
					</ul>
				</div>
				<hr />
				<div id="preprints">
					<h2>Preprints</h2>
					<ul>
						<li>
							<b>Compressed Communication for Distributed Training: Adaptive Methods and System.</b><br/>
							Yuchen Zhong, <b>Cong Xie</b>, Shuai Zheng, Haibin Lin.
							<br/>
							<a href="https://arxiv.org/abs/2105.07829">https://arxiv.org/abs/2105.07829</a>
							<p> </p>
							<br/>
						</li>
						<li>
							<b>Phocas: dimensional Byzantine-resilient stochastic gradient descent.</b><br/>
							<b>Cong Xie</b>
							, Sanmi Koyejo, and Indranil Gupta. 
							<br/>
							<a href="https://arxiv.org/abs/1805.09682">https://arxiv.org/abs/1805.09682</a>
							<p> </p>
							<br/>
						</li>
						<li>
							<b>Distributed Power-Law Graph Computing: Theoretical and Empirical Analysis.</b><br/>
							<b>Cong Xie</b>
							, Ling Yan, Xiao-Fan Niu, Wuxuan Jiang, Wu-Jun Li, and Zhihua Zhang. 
							<br/>
							<a href="papers/powerlore.pdf">Long Version</a>,
							<a href="https://github.com/xcgoner/powerlore">Code</a>
							<p> </p>
							<br/>
						</li>
						<li>
							<b>S-PowerGraph: Streaming Graph Partitioning for Natural Graphs by Vertex-Cut. </b><br/>
							<b>Cong Xie</b>
							, Wu-Jun Li, and Zhihua Zhang. 
							<br/>
							<a href="http://arxiv.org/abs/1511.02586">http://arxiv.org/abs/1511.02586</a>
							<p> </p>
							<br/>
						</li>
						<li>
							<b>A New Relaxation Approach to Normalized Hypergraph Cut.</b><br/>
							<b>Cong Xie</b>
							, Wu-Jun Li, and Zhihua Zhang.  
							<br/>
							<a href="http://arxiv.org/abs/1511.02595">http://arxiv.org/abs/1511.02595</a>
							<p> </p>
							<br/>
						</li>
					</ul>
				</div>
				<hr />
				<div id="service">
					<h2>Academic Service</h2>
					<h3>Journal Reviewer</h3>
					<ul>
						<li>
							Journal of Machine Learning Research (JNLR)
						</li>
						<li>
							ACM Transactions on Autonomous and Adaptive Systems (TAAS)
						</li>
						<li>
							IEEE Transactions on Signal Processing
						</li>
						<li>
							IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
						</li>
						<li>
							Journal of Computer Science and Technology (JCST)
						</li>
						<li>
							Transactions on Signal and Information Processing over Networks (SIPN)
						</li>
						<li>
							Transactions on Network Science and Engineering (TNSE)
						</li>
						<li>
							Frontiers in Artificial Intelligence
						</li>
						<li>
							Journal of Systems Architecture
						</li>
						<li>
							Transactions on Knowledge Discovery from Data (TKDD)
						</li>
						<li>
							Distributed Computing
						</li>
						<li>
							IEEE Transactions on Computers (TC)
						</li>
						<li>
							IEEE Access
						</li>
						<li> 
							TMLR
						</li>
					</ul>
					<h3>Conference Reviewer</h3>
					<ul>
						<li>
							MLSys, ICML, NeurIPS, AISTATS, ICLR, UAI, DISC, AAAI, IJCAI
						</li>
					</ul>
				</div>
				<hr />
				<div id="honor">
					<h2>Honors & Awards</h2>
					<ul>
						<li>
							J.P. Morgan 2020 AI Research PhD Fellowship Awards
						</li>
						<li>
							National Scholarship (Top 2%)
						</li>
						<li>
							3rd place in IEEE PHM 2015 Data Challenge (<a href="http://phmdatachallenge.freeforums.net/thread/22/final-scores-data-challenge-2015">Leaderboard</a>)
						</li>
						<li>
							SJTU Academic Excellence Scholarship Class-B (Top 10%)
						</li>
						<li>
							SJTU Academic Excellence Scholarship Class-C (2 Times, Top 20%)
						</li>
						<li>
							2nd provincial-level in China Undergraduate Mathematical Contest in Modeling
						</li>
					</ul>
				</div>
			</div>
		</div>
	</body>
</html>
